import gymnasium as gym
import numpy as np
import random
import pickle
import kymnasium as kym
import copy


class TrainedAgent(kym.Agent):
    def __init__(self, PI):
        self.PI = PI  

    def act(self, obs, info):
        row, col = np.argwhere(obs >999)[0]
        dir = obs[row][col]-1000
        state = (row * 26 + col) * 4 + dir
        return self.PI[state]
    
    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, path):
        with open(path, 'rb') as f:
            return pickle.load(f)

COORDS = {
    0: (0, 1),   # right
    1: (1, 0),   # down
    2: (0, -1),  # left
    3: (-1, 0),  # up
}


def step_state(row, col, dir, action,tile_map):
    if action == 0:  # turn left
        dir = (dir - 1) % 4
    elif action == 1:  # turn right
        dir = (dir + 1) % 4
    elif action == 2:  # move forward
        d_row, d_col = COORDS[dir]
        row += d_row
        col += d_col
        row = min(max(row, 1), 24)
        col = min(max(col, 1), 24)
        if tile_map[row][col]==900:
            row -= d_row
            col -= d_col
            row = min(max(row, 1), 24)
            col = min(max(col, 1), 24)
    
    return row, col, dir


def estimate_value(row, col, dir, action, gamma, V, tile_map):
    new_row, new_col, new_dir = step_state(row, col, dir, action,tile_map)
    new_state = (new_row * 26 + new_col) * 4 + new_dir
    tile = int(tile_map[new_row, new_col])

    # Reward
    if tile == 900:  
        reward = -10
    elif tile == 250:
        reward = -1
    elif tile == 810:  
        reward = 100000
    else:
        reward = -1  

    v = reward + gamma * V[new_state]
    return v



def train():
    env = gym.make('kymnasium/GridAdventure-Crossing-26x26-v0', render_mode='rgb_array', bgm=False)
    obs, info = env.reset()
    tile_map = obs.copy()

    num_states = 26 * 26 * 4
    gamma = 0.99
    phi = 1e-3
    max_episodes = 1000

    V = np.full(num_states, 0, dtype=float)
    PI = np.random.choice([0, 1, 2], size=num_states)
    PI_prev = np.full(num_states, 0)

    episode = 0
    while (PI!= PI_prev).any():
        episode += 1
        if episode > max_episodes:
            break

        while True:
            delta = 0
            for state in range(num_states):
                row = (state // 4) // 26
                col = (state // 4) % 26
                dir = state % 4
                v_prev = V[state]
                V[state] = estimate_value(row, col, dir, PI[state], gamma, V, tile_map)
                delta = max(delta, abs(V[state] - v_prev))
            if delta < phi:
                break

        PI_prev = PI.copy()
        for state in range(num_states):
            row = (state // 4) // 26
            col = (state // 4) % 26
            dir = state % 4
            best_v = -1e10
            best_a = -1
            for action in [0, 1, 2]:
                v = estimate_value(row, col, dir, action, gamma, V, tile_map)
                if v > best_v:
                    best_v = v
                    best_a = action
            PI[state] = best_a

    agent = TrainedAgent(PI)
    agent.save('./gridcrossing2.pkl')
    print("Train")

    train()


